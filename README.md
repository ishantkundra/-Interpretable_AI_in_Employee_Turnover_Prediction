ğŸ§  Project Overview
	â€¢	Course: CSCE 689 â€“ Special Topics in Human-AI Interaction
	â€¢	Title: Interpretable AI in Employee Turnover Prediction
	â€¢	Focus: Using LIME and SHAP to explain model predictions and assess fairness
	â€¢	Model: Random Forest Classifier
	â€¢	Dataset: Employee attrition dataset
	â€¢	XAI Techniques: LIME (local), SHAP (global)
	â€¢	Insights: Feature influence, calibration, bias by age/gender, compute time comparison

â¸»

ğŸ“¦ Suggested Repository Name

You can go with:
	â€¢	lime-shap-employee-explainability
	â€¢	interpretable-ai-turnover
	â€¢	xai-employee-attrition
	â€¢	âœ… Recommended: employee-attrition-xai-lime-shap

â¸»

ğŸ“ GitHub-Ready README.md with <pre> Structure

# Explainable AI for Employee Turnover Prediction: LIME vs SHAP

This project explores **interpretable AI techniques** to explain the predictions of a Random Forest model trained on employee attrition data. Using **LIME** (Local Interpretable Model-agnostic Explanations) and **SHAP** (SHapley Additive exPlanations), the study compares local and global feature attributions and analyzes model fairness with respect to **age** and **gender**.

Developed as part of **CSCE 689: Human-AI Interaction** at **Texas A&M University**.

---

## ğŸ“ Repository Structure

<pre>
employee-attrition-xai-lime-shap/
â”‚
â”œâ”€â”€ Code/                      # Jupyter Notebook with full implementation
â”‚   â””â”€â”€ CSCE 689 Human AI - Interaction HW- 02 ISHANT KUNDRA.ipynb
â”‚
â”œâ”€â”€ Report/                    # Final PDF report with visualizations and analysis
â”‚   â””â”€â”€ CSCE 689- Ishant Kundra.pdf
â”‚
â”œâ”€â”€ Dataset/                   # Employee attrition dataset
â”‚   â””â”€â”€ Employee.csv
â”‚
â”œâ”€â”€ Problem_Questtion/                # Original assignment brief for reference
â”‚   â””â”€â”€ Assignment 2.pdf
â”‚
â””â”€â”€ README.md                  # Youâ€™re here!
</pre>

---

## ğŸ“Œ Project Goals

- Train a machine learning model (Random Forest) to predict employee attrition
- Apply LIME and SHAP for model interpretation
- Visualize feature influences and compare explanations
- Evaluate fairness of predictions across **age** and **gender**
- Compare **execution time** and effectiveness of LIME vs SHAP

---

## ğŸ§ª Methodology

- Data preprocessing & encoding of categorical features
- Split: 70% train / 30% test
- RandomForestClassifier from `sklearn`
- Feature importance measured via:
  - LIME (individual predictions)
  - SHAP (global impact)

---

## ğŸ“Š Key Results

- **LIME**: Explained individual predictions (e.g., influence of `City`, `Education`, `JoiningYear`)
- **SHAP**: Global feature importance showed `JoiningYear` and `PaymentTier` as key predictors
- **Bias Analysis**:
  - Higher attrition prediction for <30 age group and females
  - Calibration curve issues for older employees and females
- **Time Comparison**:
  - LIME: ~0.23 seconds (fast, instance-level)
  - SHAP: ~13.8 seconds (slower, global-level)

---

## ğŸ§  Insights

| Technique | Type       | Speed      | Best For                    |
|-----------|------------|------------|-----------------------------|
| LIME      | Local      | Very Fast  | Real-time decisions         |
| SHAP      | Global     | Slower     | Audits, fairness evaluation |

Both are **model-agnostic**, powerful tools for explainability.

---

## ğŸ“ˆ Visuals Included

- LIME bar charts for instance-level interpretation
- SHAP summary plots for global feature impact
- Calibration curves for fairness across groups

---

## âš™ï¸ Tools & Libraries

- Python
- Scikit-learn
- LIME
- SHAP
- Pandas, Matplotlib

---

## ğŸ‘¨â€ğŸ’» Author

**Ishant Kundra**  
M.S. Computer Science, Texas A&M University  
ğŸ“¬ [ishantkundra9@gmail.com]

