# Explainable AI for Employee Turnover Prediction: LIME vs SHAP

This project explores **interpretable AI techniques** to explain the predictions of a Random Forest model trained on employee attrition data. Using **LIME** (Local Interpretable Model-agnostic Explanations) and **SHAP** (SHapley Additive exPlanations), the study compares local and global feature attributions and analyzes model fairness with respect to **age** and **gender**.

Developed as part of **CSCE 689: Human-AI Interaction** at **Texas A&M University**.

---

## ğŸ§  Project Overview

- **Course**: CSCE 689 â€“ Special Topics in Human-AI Interaction  
- **Title**: Interpretable AI in Employee Turnover Prediction  
- **Focus**: Using LIME and SHAP to explain model predictions and assess fairness  
- **Model**: Random Forest Classifier  
- **Dataset**: Employee attrition dataset  
- **XAI Techniques**: LIME (local), SHAP (global)  
- **Insights**: Feature influence, calibration, bias by age/gender, compute time comparison  

---

## ğŸ“¦ Suggested Repository Name

You can go with:
- `lime-shap-employee-explainability`  
- `interpretable-ai-turnover`  
- `xai-employee-attrition`  
- âœ… **Recommended**: `employee-attrition-xai-lime-shap`

---

## ğŸ“ Repository Structure

<pre>
employee-attrition-xai-lime-shap/
â”‚
â”œâ”€â”€ Code/                         # Jupyter Notebook with full implementation
â”‚   â””â”€â”€ CSCE 689 Human AI - Interaction HW- 02 ISHANT KUNDRA.ipynb
â”‚
â”œâ”€â”€ Report/                       # Final PDF report with visualizations and analysis
â”‚   â””â”€â”€ CSCE 689- Ishant Kundra.pdf
â”‚
â”œâ”€â”€ Dataset/                      # Employee attrition dataset
â”‚   â””â”€â”€ Employee.csv
â”‚
â”œâ”€â”€ Problem_Question/             # Original assignment brief for reference
â”‚   â””â”€â”€ Assignment 2.pdf
â”‚
â””â”€â”€ README.md                     # Youâ€™re here!
</pre>

---

## ğŸ“Œ Project Goals

- Train a machine learning model (Random Forest) to predict employee attrition  
- Apply LIME and SHAP for model interpretation  
- Visualize feature influences and compare explanations  
- Evaluate fairness of predictions across **age** and **gender**  
- Compare **execution time** and effectiveness of LIME vs SHAP  

---

## ğŸ§ª Methodology

- Data preprocessing & encoding of categorical features  
- Split: 70% train / 30% test  
- RandomForestClassifier from `sklearn`  
- Feature importance measured via:
  - **LIME**: individual, local explanations  
  - **SHAP**: global feature impact overview  

---

## ğŸ“Š Key Results

- **LIME**: Explained individual predictions (e.g., influence of `City`, `Education`, `JoiningYear`)  
- **SHAP**: Global feature importance showed `JoiningYear` and `PaymentTier` as key predictors  
- **Bias Analysis**:
  - Higher attrition prediction for <30 age group and females  
  - Calibration curve issues for older employees and females  
- **Time Comparison**:
  - LIME: ~0.23 seconds (fast, instance-level)  
  - SHAP: ~13.8 seconds (slower, global-level)  

---

## ğŸ§  Insights

| Technique | Type   | Speed     | Best For                  |
|-----------|--------|-----------|---------------------------|
| LIME      | Local  | Very Fast | Real-time decisions       |
| SHAP      | Global | Slower    | Audits, fairness analysis |

Both are **model-agnostic**, powerful tools for explainability.

---

## ğŸ“ˆ Visuals Included

- LIME bar charts for instance-level interpretation  
- SHAP summary plots for global feature impact  
- Calibration curves for fairness across demographic groups  

---

## âš™ï¸ Tools & Libraries

- Python  
- Scikit-learn  
- LIME  
- SHAP  
- Pandas, Matplotlib  

---

## ğŸ‘¨â€ğŸ’» Author

**Ishant Kundra**  
M.S. Computer Science, Texas A&M University  
ğŸ“¬ ishantkundra9@gmail.com
